{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming\n",
    "\n",
    "Real-time recommendation engine with:\n",
    "- Session windows (7-day gap for historical data - because we do not have in minute wise or hourly data)\n",
    "- Job A: Pair generation with Redis pipelining\n",
    "- Job B: Similarity computation (background thread)\n",
    "- Job C: Recommendation generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "CHECKPOINT_DIR = \"../checkpoints/streaming\"\n",
    "\n",
    "# clear checkpoints for fresh run\n",
    "if os.path.exists(CHECKPOINT_DIR):\n",
    "    shutil.rmtree(CHECKPOINT_DIR)\n",
    "    print(f\"Cleared: {CHECKPOINT_DIR}\")\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # kafka\n",
    "    kafka_servers: str = \"kafka-broker-1:19092,kafka-broker-2:19092,kafka-broker-3:19092\"\n",
    "    kafka_topic: str = \"movielens-ratings\"\n",
    "    \n",
    "    # spark\n",
    "    trigger_interval: str = \"10 seconds\"\n",
    "    max_offsets: int = 500000\n",
    "    \n",
    "    # streaming\n",
    "    session_gap: str = \"7 days\"\n",
    "    watermark_delay: str = \"1 day\"\n",
    "    max_session_items: int = 50\n",
    "    rating_threshold: float = 3.5\n",
    "    \n",
    "    # redis\n",
    "    pipeline_batch: int = 5000\n",
    "    top_k_neighbors: int = 500\n",
    "    top_k_recs: int = 10\n",
    "    \n",
    "    # similarity\n",
    "    similarity_method: str = \"conditional\"  # conditional, jaccard, cosine, pmi\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# background job B\n",
    "BACKGROUND_JOB_B = True\n",
    "BACKGROUND_INTERVAL = 60\n",
    "\n",
    "print(f\"Kafka: {config.kafka_servers}\")\n",
    "print(f\"Session gap: {config.session_gap}, Max items: {config.max_session_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redis Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "from redis.cluster import RedisCluster, ClusterNode\n",
    "\n",
    "def get_redis():\n",
    "    nodes = [\n",
    "        ClusterNode(\"redis-node-1\", 6379),\n",
    "        ClusterNode(\"redis-node-2\", 6379),\n",
    "        ClusterNode(\"redis-node-3\", 6379),\n",
    "    ]\n",
    "    try:\n",
    "        r = RedisCluster(startup_nodes=nodes, decode_responses=True)\n",
    "        r.ping()\n",
    "        print(\"Connected to Redis Cluster\")\n",
    "        return r\n",
    "    except Exception as e:\n",
    "        print(f\"Cluster failed ({e}), trying standalone...\")\n",
    "        r = redis.Redis(host=\"redis-node-1\", port=6379, decode_responses=True)\n",
    "        r.ping()\n",
    "        print(\"Connected to Redis (standalone)\")\n",
    "        return r\n",
    "\n",
    "redis_conn = get_redis()\n",
    "\n",
    "# clear existing data\n",
    "for pattern in [\"user:*\", \"item:*\", \"metrics:*\"]:\n",
    "    for key in redis_conn.scan_iter(match=pattern, count=1000):\n",
    "        redis_conn.delete(key)\n",
    "print(\"Redis cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, LongType, StringType\n",
    "\n",
    "# detect spark version for correct kafka package\n",
    "import pyspark\n",
    "spark_version = pyspark.__version__\n",
    "scala_version = \"2.13\" if spark_version.startswith(\"4.\") else \"2.12\"\n",
    "kafka_pkg = f\"org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"RecommendationEngine\")\n",
    "    .master(os.getenv(\"SPARK_MASTER_URL\", \"spark://spark-master:7077\"))\n",
    "    .config(\"spark.jars.packages\", kafka_pkg)\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"24\")\n",
    "    .config(\"spark.streaming.backpressure.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"Spark {spark_version}, Scala {scala_version}\")\n",
    "\n",
    "# schema for rating events\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"movie_id\", IntegerType()),\n",
    "    StructField(\"rating\", FloatType()),\n",
    "    StructField(\"original_timestamp\", LongType()),\n",
    "    StructField(\"event_timestamp\", LongType()),\n",
    "    StructField(\"event_id\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job A: Pair Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobA:\n",
    "    \"\"\"Session-based pair generation with pipelining.\"\"\"\n",
    "    \n",
    "    def __init__(self, redis_client, cfg):\n",
    "        self.r = redis_client\n",
    "        self.cfg = cfg\n",
    "        self.batch_count = 0\n",
    "        self.total_pairs = 0\n",
    "    \n",
    "    def _sample_session(self, events):\n",
    "        \"\"\"Smart sampling: keep first/last items, sample middle.\"\"\"\n",
    "        max_items = self.cfg.max_session_items\n",
    "        if len(events) <= max_items:\n",
    "            return events\n",
    "        \n",
    "        keep = min(10, max_items // 4)\n",
    "        first = events[:keep]\n",
    "        last = events[-keep:]\n",
    "        middle = events[keep:-keep]\n",
    "        \n",
    "        sample_size = max_items - 2 * keep\n",
    "        if len(middle) > sample_size:\n",
    "            sampled = sorted(random.sample(middle, sample_size), key=lambda x: x['event_time'])\n",
    "        else:\n",
    "            sampled = middle\n",
    "        \n",
    "        return first + sampled + last\n",
    "    \n",
    "    def process(self, batch_df, batch_id):\n",
    "        \"\"\"Process a batch of sessions.\"\"\"\n",
    "        start = time.time()\n",
    "        rows = batch_df.collect()\n",
    "        \n",
    "        if not rows:\n",
    "            return\n",
    "        \n",
    "        # parse sessions\n",
    "        user_updates = defaultdict(set)\n",
    "        cooc_updates = defaultdict(lambda: defaultdict(float))\n",
    "        \n",
    "        for row in rows:\n",
    "            user_id = row['user_id']\n",
    "            events = row['events'] or []\n",
    "            \n",
    "            # filter positive ratings\n",
    "            positive = [e for e in events if e['rating'] >= self.cfg.rating_threshold]\n",
    "            if len(positive) < 2:\n",
    "                continue\n",
    "            \n",
    "            # sample if needed\n",
    "            positive = self._sample_session(positive)\n",
    "            \n",
    "            # update user history\n",
    "            for e in positive:\n",
    "                user_updates[user_id].add(e['movie_id'])\n",
    "            \n",
    "            # generate pairs with proximity weighting\n",
    "            n = len(positive)\n",
    "            for i in range(n):\n",
    "                for j in range(i + 1, n):\n",
    "                    m1, m2 = positive[i]['movie_id'], positive[j]['movie_id']\n",
    "                    weight = 1.0 / (1 + abs(j - i) * 0.1)  # closer = higher weight\n",
    "                    if m1 < m2:\n",
    "                        cooc_updates[m1][m2] += weight\n",
    "                    else:\n",
    "                        cooc_updates[m2][m1] += weight\n",
    "        \n",
    "        # write to redis with pipelining\n",
    "        pipe = self.r.pipeline(transaction=False)\n",
    "        pipe_count = 0\n",
    "        \n",
    "        # user histories\n",
    "        for uid, movies in user_updates.items():\n",
    "            pipe.sadd(f\"user:history:{uid}\", *[str(m) for m in movies])\n",
    "            pipe_count += 1\n",
    "            if pipe_count >= self.cfg.pipeline_batch:\n",
    "                pipe.execute()\n",
    "                pipe = self.r.pipeline(transaction=False)\n",
    "                pipe_count = 0\n",
    "        \n",
    "        # co-occurrence (symmetric)\n",
    "        pair_count = 0\n",
    "        for m1, neighbors in cooc_updates.items():\n",
    "            for m2, weight in neighbors.items():\n",
    "                pipe.zincrby(f\"item:cooc:{m1}\", weight, str(m2))\n",
    "                pipe.zincrby(f\"item:cooc:{m2}\", weight, str(m1))\n",
    "                pipe_count += 2\n",
    "                pair_count += 1\n",
    "                if pipe_count >= self.cfg.pipeline_batch:\n",
    "                    pipe.execute()\n",
    "                    pipe = self.r.pipeline(transaction=False)\n",
    "                    pipe_count = 0\n",
    "        \n",
    "        if pipe_count > 0:\n",
    "            pipe.execute()\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        self.batch_count += 1\n",
    "        self.total_pairs += pair_count\n",
    "        \n",
    "        # metrics\n",
    "        self.r.hset(\"metrics:job_a\", mapping={\n",
    "            \"batch_count\": self.batch_count,\n",
    "            \"total_pairs\": self.total_pairs,\n",
    "            \"last_sessions\": len(rows),\n",
    "            \"last_time_ms\": int(elapsed * 1000)\n",
    "        })\n",
    "        \n",
    "        print(f\"[JobA] batch={batch_id} sessions={len(rows)} pairs={pair_count} time={elapsed:.1f}s\")\n",
    "        return list(user_updates.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job B: Similarity Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobB:\n",
    "    \"\"\"Compute item similarity from co-occurrence.\"\"\"\n",
    "    \n",
    "    def __init__(self, redis_client, cfg):\n",
    "        self.r = redis_client\n",
    "        self.cfg = cfg\n",
    "        self.run_count = 0\n",
    "    \n",
    "    def compute(self):\n",
    "        \"\"\"Update similarity matrix for all items.\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        # get all items with co-occurrence data\n",
    "        items = [k.split(\":\")[-1] for k in self.r.scan_iter(match=\"item:cooc:*\", count=1000)]\n",
    "        if not items:\n",
    "            return\n",
    "        \n",
    "        # get item counts for normalization\n",
    "        item_counts = {}\n",
    "        pipe = self.r.pipeline(transaction=False)\n",
    "        for item in items:\n",
    "            pipe.zcard(f\"item:cooc:{item}\")\n",
    "        counts = pipe.execute()\n",
    "        for item, count in zip(items, counts):\n",
    "            if count:\n",
    "                total = sum(float(s) for _, s in self.r.zrange(f\"item:cooc:{item}\", 0, -1, withscores=True))\n",
    "                item_counts[item] = total\n",
    "        \n",
    "        # compute similarities\n",
    "        method = self.cfg.similarity_method\n",
    "        items_processed = 0\n",
    "        \n",
    "        for item in items:\n",
    "            coocs = self.r.zrevrange(f\"item:cooc:{item}\", 0, self.cfg.top_k_neighbors - 1, withscores=True)\n",
    "            if not coocs:\n",
    "                continue\n",
    "            \n",
    "            similarities = {}\n",
    "            count_i = item_counts.get(item, 1)\n",
    "            \n",
    "            for other, cooc in coocs:\n",
    "                count_j = item_counts.get(other, 1)\n",
    "                \n",
    "                if method == \"conditional\":\n",
    "                    sim = cooc / count_i if count_i > 0 else 0\n",
    "                elif method == \"jaccard\":\n",
    "                    sim = cooc / (count_i + count_j - cooc) if (count_i + count_j - cooc) > 0 else 0\n",
    "                elif method == \"cosine\":\n",
    "                    sim = cooc / math.sqrt(count_i * count_j) if count_i > 0 and count_j > 0 else 0\n",
    "                else:  # pmi\n",
    "                    total = sum(item_counts.values()) or 1\n",
    "                    pmi = math.log((cooc * total) / (count_i * count_j + 1) + 1)\n",
    "                    sim = max(0, pmi)\n",
    "                \n",
    "                similarities[other] = sim\n",
    "            \n",
    "            if similarities:\n",
    "                self.r.delete(f\"item:sim:{item}\")\n",
    "                self.r.zadd(f\"item:sim:{item}\", similarities)\n",
    "                items_processed += 1\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        self.run_count += 1\n",
    "        \n",
    "        self.r.hset(\"metrics:job_b\", mapping={\n",
    "            \"run_count\": self.run_count,\n",
    "            \"items_processed\": items_processed,\n",
    "            \"last_time_ms\": int(elapsed * 1000),\n",
    "            \"method\": method\n",
    "        })\n",
    "        \n",
    "        print(f\"[JobB] items={items_processed} method={method} time={elapsed:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job C: Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobC:\n",
    "    \"\"\"Generate recommendations from similarity matrix.\"\"\"\n",
    "    \n",
    "    def __init__(self, redis_client, cfg):\n",
    "        self.r = redis_client\n",
    "        self.cfg = cfg\n",
    "        self.run_count = 0\n",
    "    \n",
    "    def generate(self, user_ids=None):\n",
    "        \"\"\"Generate recommendations for users.\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        # get users to process\n",
    "        if user_ids is None:\n",
    "            user_ids = [k.split(\":\")[-1] for k in self.r.scan_iter(match=\"user:history:*\", count=1000)]\n",
    "        \n",
    "        if not user_ids:\n",
    "            return\n",
    "        \n",
    "        users_updated = 0\n",
    "        \n",
    "        for uid in user_ids:\n",
    "            # get user history\n",
    "            history = self.r.smembers(f\"user:history:{uid}\")\n",
    "            if not history:\n",
    "                continue\n",
    "            \n",
    "            # aggregate similarity scores\n",
    "            scores = defaultdict(float)\n",
    "            for movie in history:\n",
    "                sims = self.r.zrevrange(f\"item:sim:{movie}\", 0, 50, withscores=True)\n",
    "                for other, score in sims:\n",
    "                    if other not in history:\n",
    "                        scores[other] += score\n",
    "            \n",
    "            # get top-k\n",
    "            if scores:\n",
    "                top = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:self.cfg.top_k_recs]\n",
    "                self.r.delete(f\"user:recs:{uid}\")\n",
    "                self.r.zadd(f\"user:recs:{uid}\", dict(top))\n",
    "                users_updated += 1\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        self.run_count += 1\n",
    "        \n",
    "        self.r.hset(\"metrics:job_c\", mapping={\n",
    "            \"run_count\": self.run_count,\n",
    "            \"users_updated\": users_updated,\n",
    "            \"last_time_ms\": int(elapsed * 1000)\n",
    "        })\n",
    "        \n",
    "        print(f\"[JobC] users={users_updated} time={elapsed:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Similarity Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackgroundRunner:\n",
    "    \"\"\"Run Job B in background thread.\"\"\"\n",
    "    \n",
    "    def __init__(self, job_b, interval):\n",
    "        self.job_b = job_b\n",
    "        self.interval = interval\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "    \n",
    "    def _run(self):\n",
    "        while self.running:\n",
    "            try:\n",
    "                self.job_b.compute()\n",
    "            except Exception as e:\n",
    "                print(f\"[BackgroundRunner] error: {e}\")\n",
    "            time.sleep(self.interval)\n",
    "    \n",
    "    def start(self):\n",
    "        self.running = True\n",
    "        self.thread = threading.Thread(target=self._run, daemon=True)\n",
    "        self.thread.start()\n",
    "        print(f\"[BackgroundRunner] started (interval={self.interval}s)\")\n",
    "    \n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        if self.thread:\n",
    "            self.thread.join(timeout=5)\n",
    "        print(\"[BackgroundRunner] stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Jobs and Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, session_window, collect_list, struct, from_unixtime\n",
    "\n",
    "# create jobs\n",
    "job_a = JobA(redis_conn, config)\n",
    "job_b = JobB(redis_conn, config)\n",
    "job_c = JobC(redis_conn, config)\n",
    "\n",
    "# background runner\n",
    "background_runner = BackgroundRunner(job_b, BACKGROUND_INTERVAL) if BACKGROUND_JOB_B else None\n",
    "\n",
    "# kafka stream\n",
    "raw_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", config.kafka_servers)\n",
    "    .option(\"subscribe\", config.kafka_topic)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"maxOffsetsPerTrigger\", config.max_offsets)\n",
    "    .option(\"failOnDataLoss\", \"false\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# parse events\n",
    "parsed = (\n",
    "    raw_stream\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"e\"))\n",
    "    .select(\n",
    "        col(\"e.user_id\").alias(\"user_id\"),\n",
    "        col(\"e.movie_id\").alias(\"movie_id\"),\n",
    "        col(\"e.rating\").alias(\"rating\"),\n",
    "        from_unixtime(col(\"e.original_timestamp\")).cast(\"timestamp\").alias(\"event_time\")\n",
    "    )\n",
    "    .filter(col(\"user_id\").isNotNull())\n",
    ")\n",
    "\n",
    "# session windows\n",
    "sessionized = (\n",
    "    parsed\n",
    "    .withWatermark(\"event_time\", config.watermark_delay)\n",
    "    .groupBy(\n",
    "        col(\"user_id\"),\n",
    "        session_window(col(\"event_time\"), config.session_gap)\n",
    "    )\n",
    "    .agg(\n",
    "        collect_list(\n",
    "            struct(\"movie_id\", \"rating\", \"event_time\")\n",
    "        ).alias(\"events\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Stream configurion Done - Ready to process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_df, batch_id):\n",
    "    \"\"\"Process batch: Job A -> Job C (Job B runs in background).\"\"\"\n",
    "    try:\n",
    "        # job A: pairs\n",
    "        affected_users = job_a.process(batch_df, batch_id)\n",
    "        \n",
    "        # job C: recommendations (only for affected users)\n",
    "        if affected_users:\n",
    "            job_c.generate(affected_users)\n",
    "    except Exception as e:\n",
    "        print(f\"[Pipeline] batch {batch_id} error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start background job B\n",
    "if background_runner:\n",
    "    background_runner.start()\n",
    "\n",
    "# start streaming\n",
    "query = (\n",
    "    sessionized\n",
    "    .writeStream\n",
    "    .foreachBatch(process_batch)\n",
    "    .trigger(processingTime=config.trigger_interval)\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_DIR}/pipeline\")\n",
    "    .queryName(\"recommendation_pipeline\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "print(f\"Streaming started: {query.name}\")\n",
    "print(f\"Active: {query.isActive}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def monitor(interval=10, iterations=None):\n",
    "    \"\"\"Show pipeline metrics.\"\"\"\n",
    "    i = 0\n",
    "    while query.isActive:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        print(f\"=== Pipeline Status ({datetime.now().strftime('%H:%M:%S')}) ===\")\n",
    "        print(f\"Query active: {query.isActive}\")\n",
    "        \n",
    "        # job metrics\n",
    "        for job in [\"job_a\", \"job_b\", \"job_c\"]:\n",
    "            m = redis_conn.hgetall(f\"metrics:{job}\")\n",
    "            if m:\n",
    "                print(f\"\\n{job.upper()}: {dict(m)}\")\n",
    "        \n",
    "        # key counts\n",
    "        cooc = len(list(redis_conn.scan_iter(match=\"item:cooc:*\", count=100)))\n",
    "        sim = len(list(redis_conn.scan_iter(match=\"item:sim:*\", count=100)))\n",
    "        recs = len(list(redis_conn.scan_iter(match=\"user:recs:*\", count=100)))\n",
    "        print(f\"\\nKeys: cooc={cooc} sim={sim} recs={recs}\")\n",
    "        \n",
    "        i += 1\n",
    "        if iterations and i >= iterations:\n",
    "            break\n",
    "        time.sleep(interval)\n",
    "\n",
    "# monitor realtime\n",
    "# monitor(interval=10, iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for streaming to complete (or interrupt manually)\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count keys\n",
    "cooc_keys = list(redis_conn.scan_iter(match=\"item:cooc:*\", count=1000))\n",
    "sim_keys = list(redis_conn.scan_iter(match=\"item:sim:*\", count=1000))\n",
    "history_keys = list(redis_conn.scan_iter(match=\"user:history:*\", count=1000))\n",
    "rec_keys = list(redis_conn.scan_iter(match=\"user:recs:*\", count=1000))\n",
    "\n",
    "print(f\"Co-occurrence keys: {len(cooc_keys)}\")\n",
    "print(f\"Similarity keys: {len(sim_keys)}\")\n",
    "print(f\"User history keys: {len(history_keys)}\")\n",
    "print(f\"Recommendation keys: {len(rec_keys)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample recommendations\n",
    "if rec_keys:\n",
    "    sample = rec_keys[0]\n",
    "    uid = sample.split(\":\")[-1]\n",
    "    recs = redis_conn.zrevrange(sample, 0, 9, withscores=True)\n",
    "    print(f\"\\nSample recommendations for user {uid}:\")\n",
    "    for movie, score in recs:\n",
    "        print(f\"  Movie {movie}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop when done\n",
    "try:\n",
    "    query.stop()\n",
    "    print(\"Query stopped\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if background_runner:\n",
    "    background_runner.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
