{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation & Benchmarking\n",
    "\n",
    "Evaluate recommendation quality:\n",
    "- Precision, Recall, NDCG, Hit Rate\n",
    "- Personalization metrics\n",
    "- Serving latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "from redis.cluster import RedisCluster, ClusterNode\n",
    "\n",
    "def get_redis():\n",
    "    nodes = [\n",
    "        ClusterNode(\"redis-node-1\", 6379),\n",
    "        ClusterNode(\"redis-node-2\", 6379),\n",
    "        ClusterNode(\"redis-node-3\", 6379),\n",
    "    ]\n",
    "    try:\n",
    "        r = RedisCluster(startup_nodes=nodes, decode_responses=True)\n",
    "        r.ping()\n",
    "        return r\n",
    "    except:\n",
    "        r = redis.Redis(host=\"redis-node-1\", port=6379, decode_responses=True)\n",
    "        r.ping()\n",
    "        return r\n",
    "\n",
    "redis_client = get_redis()\n",
    "print(\"Connected to Redis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data/processed\")\n",
    "RESULTS_DIR = Path(\"../results\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# load ground truth\n",
    "with open(DATA_DIR / \"ground_truth.pkl\", 'rb') as f:\n",
    "    ground_truth = pickle.load(f)\n",
    "\n",
    "# load stats\n",
    "with open(DATA_DIR / \"dataset_stats.pkl\", 'rb') as f:\n",
    "    stats = pickle.load(f)\n",
    "\n",
    "print(f\"Ground truth users: {len(ground_truth):,}\")\n",
    "print(f\"Dataset stats: {stats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check redis data\n",
    "rec_keys = list(redis_client.scan_iter(match=\"user:recs:*\", count=1000))\n",
    "history_keys = list(redis_client.scan_iter(match=\"user:history:*\", count=1000))\n",
    "cooc_keys = list(redis_client.scan_iter(match=\"item:cooc:*\", count=1000))\n",
    "\n",
    "print(f\"Recommendations: {len(rec_keys):,}\")\n",
    "print(f\"User histories: {len(history_keys):,}\")\n",
    "print(f\"Co-occurrence: {len(cooc_keys):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(recommended, relevant, k=10):\n",
    "    \"\"\"Fraction of recommendations that are relevant.\"\"\"\n",
    "    recs = set(recommended[:k])\n",
    "    hits = len(recs & relevant)\n",
    "    return hits / k\n",
    "\n",
    "def recall_at_k(recommended, relevant, k=10):\n",
    "    \"\"\"Fraction of relevant items that were recommended.\"\"\"\n",
    "    recs = set(recommended[:k])\n",
    "    hits = len(recs & relevant)\n",
    "    return hits / len(relevant) if relevant else 0\n",
    "\n",
    "def ndcg_at_k(recommended, relevant, k=10):\n",
    "    \"\"\"Normalized discounted cumulative gain.\"\"\"\n",
    "    dcg = sum(1.0 / np.log2(i + 2) for i, item in enumerate(recommended[:k]) if item in relevant)\n",
    "    idcg = sum(1.0 / np.log2(i + 2) for i in range(min(k, len(relevant))))\n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "def hit_rate(recommended, relevant, k=10):\n",
    "    \"\"\"Whether any recommendation is relevant.\"\"\"\n",
    "    return 1 if set(recommended[:k]) & relevant else 0\n",
    "\n",
    "def mrr(recommended, relevant):\n",
    "    \"\"\"Mean reciprocal rank.\"\"\"\n",
    "    for i, item in enumerate(recommended):\n",
    "        if item in relevant:\n",
    "            return 1.0 / (i + 1)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(redis_client, user_id, k=10):\n",
    "    \"\"\"Get top-k recommendations for user.\"\"\"\n",
    "    recs = redis_client.zrevrange(f\"user:recs:{user_id}\", 0, k-1)\n",
    "    return [int(r) for r in recs]\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"Jaccard similarity between two sets.\"\"\"\n",
    "    if not set1 or not set2:\n",
    "        return 0\n",
    "    return len(set1 & set2) / len(set1 | set2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_quality(redis_client, ground_truth, k=10, sample_size=5000):\n",
    "    \"\"\"Evaluate recommendation quality metrics.\"\"\"\n",
    "    \n",
    "    # sample users\n",
    "    users = list(ground_truth.keys())\n",
    "    if len(users) > sample_size:\n",
    "        users = random.sample(users, sample_size)\n",
    "    \n",
    "    metrics = {\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'ndcg': [],\n",
    "        'hit_rate': [],\n",
    "        'mrr': []\n",
    "    }\n",
    "    \n",
    "    evaluated = 0\n",
    "    no_recs = 0\n",
    "    \n",
    "    for uid in users:\n",
    "        recs = get_recommendations(redis_client, uid, k)\n",
    "        if not recs:\n",
    "            no_recs += 1\n",
    "            continue\n",
    "        \n",
    "        relevant = ground_truth[uid]\n",
    "        \n",
    "        metrics['precision'].append(precision_at_k(recs, relevant, k))\n",
    "        metrics['recall'].append(recall_at_k(recs, relevant, k))\n",
    "        metrics['ndcg'].append(ndcg_at_k(recs, relevant, k))\n",
    "        metrics['hit_rate'].append(hit_rate(recs, relevant, k))\n",
    "        metrics['mrr'].append(mrr(recs, relevant))\n",
    "        evaluated += 1\n",
    "    \n",
    "    results = {\n",
    "        'k': k,\n",
    "        'users_evaluated': evaluated,\n",
    "        'users_no_recs': no_recs,\n",
    "        'coverage': evaluated / len(users) if users else 0\n",
    "    }\n",
    "    \n",
    "    for name, values in metrics.items():\n",
    "        if values:\n",
    "            results[f'{name}@{k}'] = np.mean(values)\n",
    "            results[f'{name}@{k}_std'] = np.std(values)\n",
    "        else:\n",
    "            results[f'{name}@{k}'] = 0\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "SAMPLE_SIZE = min(5000, len(ground_truth))\n",
    "\n",
    "quality = evaluate_quality(redis_client, ground_truth, k=K, sample_size=SAMPLE_SIZE)\n",
    "\n",
    "print(f\"QUALITY METRICS (K={K})\")\n",
    "print(f\"\\nUsers evaluated: {quality['users_evaluated']:,}\")\n",
    "print(f\"\\nUsers w/o recs: {quality['users_no_recs']:,}\")\n",
    "print(f\"\\nCoverage: {quality['coverage']:.1%}\")\n",
    "print(f\"\\nPrecision@{K}: {quality[f'precision@{K}']:.4f}\")\n",
    "print(f\"\\nRecall@{K}: {quality[f'recall@{K}']:.4f}\")\n",
    "print(f\"\\nNDCG@{K}: {quality[f'ndcg@{K}']:.4f}\")\n",
    "print(f\"\\nHit Rate@{K}: {quality[f'hit_rate@{K}']:.4f}\")\n",
    "print(f\"\\nMRR: {quality[f'mrr@{K}']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Personalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_personalization(redis_client, user_ids, k=10, num_pairs=5000):\n",
    "    \"\"\"Measure how different recommendations are across users.\"\"\"\n",
    "    \n",
    "    # get recommendations for sample of users\n",
    "    user_recs = {}\n",
    "    for uid in user_ids[:5000]:\n",
    "        recs = get_recommendations(redis_client, uid, k)\n",
    "        if recs:\n",
    "            user_recs[uid] = set(recs)\n",
    "    \n",
    "    if len(user_recs) < 2:\n",
    "        return {'inter_user_similarity': 0, 'personalization': 1}\n",
    "    \n",
    "    # compute pairwise similarities\n",
    "    users = list(user_recs.keys())\n",
    "    similarities = []\n",
    "    \n",
    "    for _ in range(min(num_pairs, len(users) * (len(users) - 1) // 2)):\n",
    "        u1, u2 = random.sample(users, 2)\n",
    "        sim = jaccard_similarity(user_recs[u1], user_recs[u2])\n",
    "        similarities.append(sim)\n",
    "    \n",
    "    mean_sim = np.mean(similarities)\n",
    "    return {\n",
    "        'inter_user_similarity': mean_sim,\n",
    "        'personalization': 1 - mean_sim,\n",
    "        'users_sampled': len(user_recs),\n",
    "        'pairs_compared': len(similarities)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get users with recommendations\n",
    "users_with_recs = [int(k.split(':')[-1]) for k in rec_keys]\n",
    "\n",
    "personalization = evaluate_personalization(redis_client, users_with_recs, k=K)\n",
    "\n",
    "print(\"PERSONALIZATION METRICS\")\n",
    "print(f\"\\nUsers sampled: {personalization['users_sampled']:,}\")\n",
    "print(f\"\\nPairs compared: {personalization['pairs_compared']:,}\")\n",
    "print(f\"\\nInter-user similarity: {personalization['inter_user_similarity']:.4f}\")\n",
    "print(f\"\\nPersonalization score: {personalization['personalization']:.4f}\")\n",
    "\n",
    "# target: inter-user similarity <= 0.30\n",
    "target = 0.30\n",
    "status = \"PASS\" if personalization['inter_user_similarity'] <= target else \"FAIL\"\n",
    "print(f\"\\nTarget (<={target}): {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latency Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_latency(redis_client, user_ids, k=10, iterations=1000):\n",
    "    \"\"\"Benchmark recommendation serving latency.\"\"\"\n",
    "    \n",
    "    latencies = []\n",
    "    sample_users = random.choices(user_ids, k=iterations)\n",
    "    \n",
    "    # warmup\n",
    "    for uid in sample_users[:100]:\n",
    "        get_recommendations(redis_client, uid, k)\n",
    "    \n",
    "    # benchmark\n",
    "    start_total = time.time()\n",
    "    for uid in sample_users:\n",
    "        start = time.perf_counter()\n",
    "        get_recommendations(redis_client, uid, k)\n",
    "        latencies.append((time.perf_counter() - start) * 1000)  # ms\n",
    "    elapsed = time.time() - start_total\n",
    "    \n",
    "    return {\n",
    "        'iterations': iterations,\n",
    "        'throughput': iterations / elapsed,\n",
    "        'latency_mean': np.mean(latencies),\n",
    "        'latency_median': np.median(latencies),\n",
    "        'latency_p95': np.percentile(latencies, 95),\n",
    "        'latency_p99': np.percentile(latencies, 99),\n",
    "        'latency_min': min(latencies),\n",
    "        'latency_max': max(latencies)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latency = benchmark_latency(redis_client, users_with_recs, k=K, iterations=1000)\n",
    "\n",
    "print(f\"Iterations: {latency['iterations']:,}\")\n",
    "print(f\"Throughput: {latency['throughput']:.0f} req/sec\")\n",
    "print(f\"\\nLatency (ms):\")\n",
    "print(f\"  Mean: {latency['latency_mean']:.3f}\")\n",
    "print(f\"  Median: {latency['latency_median']:.3f}\")\n",
    "print(f\"  P95: {latency['latency_p95']:.3f}\")\n",
    "print(f\"  P99: {latency['latency_p99']:.3f}\")\n",
    "print(f\"  Min: {latency['latency_min']:.3f}\")\n",
    "print(f\"  Max: {latency['latency_max']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Quality Metrics Bar Chart\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "metrics = ['Precision', 'Recall', 'NDCG', 'Hit Rate']\n",
    "values = [quality[f'precision@{K}'], quality[f'recall@{K}'], \n",
    "          quality[f'ndcg@{K}'], quality[f'hit_rate@{K}']]\n",
    "colors = ['#2ecc71', '#3498db', '#9b59b6', '#e74c3c']\n",
    "\n",
    "bars = ax.bar(metrics, values, color=colors, edgecolor='black', linewidth=1.2)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title(f'Recommendation Quality Metrics (K={K})', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, max(values) * 1.2)\n",
    "\n",
    "for bar, v in zip(bars, values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, v + 0.01, f'{v:.3f}', \n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'figure1_quality_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {RESULTS_DIR / 'figure1_quality_metrics.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: Latency Under Load\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def measure_latency_at_load(redis_client, user_ids, num_requests, num_threads):\n",
    "    \"\"\"Measure latency with concurrent requests.\"\"\"\n",
    "    latencies = []\n",
    "    sample_users = random.choices(user_ids, k=num_requests)\n",
    "    \n",
    "    def fetch_rec(uid):\n",
    "        t0 = time.perf_counter()\n",
    "        get_recommendations(redis_client, uid, 10)\n",
    "        return (time.perf_counter() - t0) * 1000\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = [executor.submit(fetch_rec, uid) for uid in sample_users]\n",
    "        for f in as_completed(futures):\n",
    "            latencies.append(f.result())\n",
    "    \n",
    "    return np.mean(latencies), np.percentile(latencies, 95)\n",
    "\n",
    "# Test at different concurrency levels\n",
    "concurrent_users = [1, 5, 10, 20, 50]\n",
    "mean_latencies = []\n",
    "p95_latencies = []\n",
    "\n",
    "print(\"Measuring latency under load...\")\n",
    "for n in concurrent_users:\n",
    "    mean_lat, p95_lat = measure_latency_at_load(redis_client, users_with_recs, 300, n)\n",
    "    mean_latencies.append(mean_lat)\n",
    "    p95_latencies.append(p95_lat)\n",
    "    print(f\"  {n} concurrent users: {mean_lat:.2f}ms mean, {p95_lat:.2f}ms p95\")\n",
    "\n",
    "# Simple bar chart\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "x = np.arange(len(concurrent_users))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, mean_latencies, width, label='Mean', color='#3498db', edgecolor='black')\n",
    "bars2 = ax.bar(x + width/2, p95_latencies, width, label='P95', color='#e74c3c', edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Concurrent Users', fontsize=12)\n",
    "ax.set_ylabel('Latency (ms)', fontsize=12)\n",
    "ax.set_title('Response Latency Under Load', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(concurrent_users)\n",
    "ax.legend()\n",
    "\n",
    "for bar in bars1:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "            f'{bar.get_height():.1f}', ha='center', va='bottom', fontsize=9)\n",
    "for bar in bars2:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "            f'{bar.get_height():.1f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'figure2_latency_throughput.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {RESULTS_DIR / 'figure2_latency_throughput.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3: Precision@K and Recall@K Curves\n",
    "k_values = [1, 3, 5, 10, 15, 20]\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "users = list(ground_truth.keys())\n",
    "sample_users = random.sample(users, min(3000, len(users)))\n",
    "\n",
    "print(\"Evaluating at different K values...\")\n",
    "for k in k_values:\n",
    "    prec_list = []\n",
    "    rec_list = []\n",
    "    \n",
    "    for uid in sample_users:\n",
    "        recs = get_recommendations(redis_client, uid, 20)  # get max needed\n",
    "        if not recs:\n",
    "            continue\n",
    "        relevant = ground_truth[uid]\n",
    "        prec_list.append(precision_at_k(recs, relevant, k))\n",
    "        rec_list.append(recall_at_k(recs, relevant, k))\n",
    "    \n",
    "    precision_scores.append(np.mean(prec_list) if prec_list else 0)\n",
    "    recall_scores.append(np.mean(rec_list) if rec_list else 0)\n",
    "    print(f\"  K={k}: Precision={precision_scores[-1]:.4f}, Recall={recall_scores[-1]:.4f}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "ax.plot(k_values, precision_scores, 'o-', color='#2ecc71', linewidth=2.5, \n",
    "        markersize=10, label='Precision@K')\n",
    "ax.plot(k_values, recall_scores, 's-', color='#3498db', linewidth=2.5, \n",
    "        markersize=10, label='Recall@K')\n",
    "\n",
    "ax.set_xlabel('K (Number of Recommendations)', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Precision and Recall at Different K Values', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(k_values)\n",
    "ax.legend(loc='center right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'figure3_precision_recall_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {RESULTS_DIR / 'figure3_precision_recall_curves.png'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
